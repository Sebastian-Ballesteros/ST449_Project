{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on paper: https://d-nb.info/1248317343/34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Output Gap Data extraction\n",
    "\n",
    "First I got the quarterly GDP for the period [Office for National Statistics]:\n",
    "https://www.ons.gov.uk/economy/grossdomesticproductgdp/timeseries/ybha/qna\n",
    "\n",
    "I got the yearly output gap [Office for Budget Responsibility (OBR)]\n",
    "https://obr.uk/public-finances-databank-2024-25/\n",
    "\n",
    "Using the quarterly estimates developed [OBR: Output gap measurement: judgement and uncertainty] I replicated the shape of the quarterly output gaps in %.\n",
    "https://obr.uk/docs/dlm_uploads/WorkingPaperNo5.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   GDP_Real (m£)  GDP_Pot (m£)  Output_gap (%) Quarter\n",
      "0         127119        130233            2.45  1987Q3\n",
      "1         129815        133288            2.68  1987Q4\n",
      "2         133283        137215            2.95  1988Q1\n",
      "3         136630        141576            3.62  1988Q2\n",
      "4         140801        145602            3.41  1988Q3\n",
      "    GDP_Real (m£)  GDP_Pot (m£)  Output_gap (%) Quarter\n",
      "77         372900        372629           -0.07  2006Q4\n",
      "78         376958        378202            0.33  2007Q1\n",
      "79         386144        387920            0.46  2007Q2\n",
      "80         389291        392366            0.79  2007Q3\n",
      "81         392244        396777            1.16  2007Q4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# Use the raw URL from the GitHub repository\n",
    "xlsx_url = \"https://raw.githubusercontent.com/guri99uy/ST449_Project/52611de9d475e711c4c917c4d5ca137427404612/outputgap.xlsx\"\n",
    "\n",
    "\n",
    "# Load the Excel file\n",
    "df_outputgap = pd.read_excel(xlsx_url, engine='openpyxl')  # Ensure you specify the 'openpyxl' engine for .xlsx files\n",
    "\n",
    "# Define a function to parse QQYYYY\n",
    "def parse_qqyyyy(qqyyyy):\n",
    "    # Extract the quarter and year\n",
    "    quarter = int(qqyyyy[1])\n",
    "    year = int(qqyyyy[2:])\n",
    "    \n",
    "    # Map the quarter to the first month of that quarter\n",
    "    quarter_start_month = {1: 1, 2: 4, 3: 7, 4: 10}\n",
    "    month = quarter_start_month[quarter]\n",
    "    \n",
    "    # Create a datetime object for the first day of the quarter\n",
    "    return dt.datetime(year, month, 1)\n",
    "\n",
    "# Apply the function to the first column 'QQYYYY' to convert it to datetime\n",
    "df_outputgap['QQYYYY'] = df_outputgap['QQYYYY'].apply(parse_qqyyyy)\n",
    "# Rename a single column, e.g., 'OldName' to 'NewName'\n",
    "df_outputgap.rename(columns={'QQYYYY': 'Date'}, inplace=True)\n",
    "\n",
    "#Get Date in Quarters\n",
    "df_outputgap['Date'] = pd.to_datetime(df_outputgap['Date'])\n",
    "df_outputgap['Quarter'] = df_outputgap['Date'].dt.to_period('Q')\n",
    "df_outputgap = df_outputgap.drop(columns=['Date'])\n",
    "\n",
    "df_outputgap['GDP_Pot (m£)'] = df_outputgap['GDP_Pot (m£)'].round(0).astype(int)\n",
    "df_outputgap['Output_gap (%)'] = df_outputgap['Output_gap (%)'].round(2)\n",
    "\n",
    "# Display the first few rows of the transformed DataFrame\n",
    "print(df_outputgap.head())\n",
    "print(df_outputgap.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Interest Rate\n",
    "Got .xlsx file from [Bank of Engalnd]\n",
    "https://www.bankofengland.co.uk/boeapps/database/Bank-Rate.asp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Every Interest rate by Bank of England:\n",
      "        Date  Interest_rate\n",
      "0 2024-11-07           4.75\n",
      "1 2024-08-01           5.00\n",
      "2 2023-08-03           5.25\n",
      "3 2023-06-22           5.00\n",
      "4 2023-05-11           4.50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# Raw URL of the Excel file\n",
    "url = \"https://raw.githubusercontent.com/guri99uy/ST449_Project/7715079b32be2ea0b9e2e77a3f7b81244f85720f/Bank_Rate.xlsx\"\n",
    "df_interest_rate = pd.read_excel(url, engine='openpyxl')\n",
    "\n",
    "\n",
    "# Rename columns for easier access (optional)\n",
    "df_interest_rate.columns = ['Date', 'Interest_rate']\n",
    "\n",
    "# Convert the 'Date_Changed' column to datetime format\n",
    "def parse_date(date_str):\n",
    "    # Handle the format '07 Nov 24' as 'DD MMM YY'\n",
    "    return dt.datetime.strptime(date_str, '%d %b %y')\n",
    "\n",
    "df_interest_rate['Date'] = df_interest_rate['Date'].apply(parse_date)\n",
    "\n",
    "# Check if 'Rate' column is string type, and process accordingly\n",
    "if df_interest_rate['Interest_rate'].dtype == 'object':\n",
    "    # Clean the 'Rate' column (replace commas with dots and convert to float)\n",
    "    df_interest_rate['Interest_rate'] = df_interest_rate['Rate'].str.replace(',', '.').astype(float)\n",
    "else:\n",
    "    # Ensure the 'Rate' column is numeric\n",
    "    df_interest_rate['Interest_rate'] = pd.to_numeric(df_interest_rate['Interest_rate'], errors='coerce')\n",
    "\n",
    "# Display the processed DataFrame\n",
    "print(\"\\nEvery Interest rate by Bank of England:\")\n",
    "print(df_interest_rate.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets process the data to: \n",
    "1. Get the quarter average\n",
    "2. Assign missing quarters with the last value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Quarter  Avg_Interest_Rate\n",
      "0  1987Q3              9.880\n",
      "1  1987Q4              8.880\n",
      "2  1988Q1              8.630\n",
      "3  1988Q2              8.080\n",
      "4  1988Q3             10.755\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_interest_rate is the DataFrame with 'Date_Changed' and 'Rate'\n",
    "# Ensure 'Date_Changed' is a datetime column\n",
    "df_interest_rate['Date'] = pd.to_datetime(df_interest_rate['Date'])\n",
    "\n",
    "# Create a column for the quarter and year as strings for grouping\n",
    "df_interest_rate['Quarter'] = df_interest_rate['Date'].dt.to_period('Q')\n",
    "\n",
    "# Group by the 'Quarter' column and calculate the average interest rate\n",
    "quarterly_avg_rate = (\n",
    "    df_interest_rate.groupby('Quarter', as_index=False)['Interest_rate']\n",
    "    .mean()\n",
    "    .rename(columns={'Interest_rate': 'Avg_Interest_Rate'})\n",
    ")\n",
    "\n",
    "full_quarters = pd.period_range('1975Q1', '2007Q4', freq='Q')\n",
    "quarterly_avg_rate['Quarter'] = pd.PeriodIndex(quarterly_avg_rate['Quarter'], freq='Q')\n",
    "quarterly_avg_rate = quarterly_avg_rate.set_index('Quarter').reindex(full_quarters)\n",
    "\n",
    "# Fill missing values with the value from the previous quarter\n",
    "quarterly_avg_rate['Avg_Interest_Rate'] = quarterly_avg_rate['Avg_Interest_Rate'].ffill()\n",
    "quarterly_avg_rate.reset_index(inplace=True)\n",
    "quarterly_avg_rate.rename(columns={'index': 'Quarter'}, inplace=True)\n",
    "\n",
    "# Filter 1997 - 2007\n",
    "Quarterly_interest_rates = quarterly_avg_rate[\n",
    "    (quarterly_avg_rate['Quarter'] >= '1987Q3') & (quarterly_avg_rate['Quarter'] <= '2007Q4')\n",
    "]\n",
    "Quarterly_interest_rates.reset_index(inplace=True)\n",
    "Quarterly_interest_rates = Quarterly_interest_rates.drop(columns=['index'])\n",
    "\n",
    "# Display\n",
    "print(Quarterly_interest_rates.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inflation\n",
    "Source?\n",
    "Relevant comments:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Quarter  GDP Deflator\n",
      "0  1987Q3       35.8724\n",
      "1  1987Q4       36.2206\n",
      "2  1988Q1       36.5950\n",
      "3  1988Q2       37.3205\n",
      "4  1988Q3       37.9849\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# GitHub raw URL for inflation\n",
    "url = \"https://raw.githubusercontent.com/guri99uy/ST449_Project/c87d1b581f0af98f2a813a9c6134160303e74883/inf_Data.csv\"\n",
    "inflation = pd.read_csv(url)\n",
    "\n",
    "# Rename columns\n",
    "inf_data = inflation.rename(columns={\"Implied GDP deflator at market prices: SA Index\": \"GDP Deflator\"})\n",
    "inf_data.rename(columns={\"Title\": \"Quarter\"}, inplace=True)\n",
    "\n",
    "# Change Quarter\n",
    "inf_data[\"Quarter\"] = inf_data[\"Quarter\"].str.replace(r\"(\\d{4})\\sQ(\\d)\", r\"\\1Q\\2\", regex=True)\n",
    "\n",
    "print(inf_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Merge relevant data\n",
    "1. Output Gap\n",
    "2. Interest rate\n",
    "3. Inflation\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Quarter  Avg_Interest_Rate  GDP_Real (m£)  GDP_Pot (m£)  Output_gap (%)  \\\n",
      "0  1987Q3              9.880         127119        130233            2.45   \n",
      "1  1987Q4              8.880         129815        133288            2.68   \n",
      "2  1988Q1              8.630         133283        137215            2.95   \n",
      "3  1988Q2              8.080         136630        141576            3.62   \n",
      "4  1988Q3             10.755         140801        145602            3.41   \n",
      "\n",
      "   GDP Deflator  \n",
      "0       35.8724  \n",
      "1       36.2206  \n",
      "2       36.5950  \n",
      "3       37.3205  \n",
      "4       37.9849  \n",
      "   Quarter  Avg_Interest_Rate  GDP_Real (m£)  GDP_Pot (m£)  Output_gap (%)  \\\n",
      "75  2006Q2               4.50         367042        366712           -0.09   \n",
      "76  2006Q3               4.75         370883        370824           -0.02   \n",
      "77  2006Q4               5.00         372900        372629           -0.07   \n",
      "78  2007Q1               5.25         376958        378202            0.33   \n",
      "79  2007Q2               5.50         386144        387920            0.46   \n",
      "\n",
      "    GDP Deflator  \n",
      "75       65.1635  \n",
      "76       65.6609  \n",
      "77       65.6670  \n",
      "78       65.7761  \n",
      "79       66.9001  \n"
     ]
    }
   ],
   "source": [
    "# Convert 'Quarter' column in all datasets to period type\n",
    "Quarterly_interest_rates['Quarter'] = pd.PeriodIndex(Quarterly_interest_rates['Quarter'], freq='Q')\n",
    "df_outputgap['Quarter'] = pd.PeriodIndex(df_outputgap['Quarter'], freq='Q')\n",
    "inf_data['Quarter'] = pd.PeriodIndex(inf_data['Quarter'], freq='Q')\n",
    "\n",
    "# Merge the datasets\n",
    "merged_df = pd.merge(Quarterly_interest_rates, df_outputgap, on='Quarter', how='inner')  # Inner join\n",
    "merged_df = pd.merge(merged_df, inf_data, on='Quarter', how='inner')  # Inner join\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(merged_df.head())\n",
    "\n",
    "\n",
    "print(merged_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict, Any, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBasedEconomyEnv:\n",
    "    def __init__(\n",
    "        self, \n",
    "        df: pd.DataFrame,\n",
    "        date_col: str = 'Quarter',\n",
    "        interest_col: str = 'Avg_Interest_Rate',\n",
    "        output_gap_col: str = 'Output gap (%)',\n",
    "        inflation_col: str = 'GDP Deflator',\n",
    "        lookback_periods: int = 2, \n",
    "        validation_split: float = 0.15\n",
    "    ):\n",
    "        \"\"\"Initialize environment with economic dataframe.\"\"\"\n",
    "        # Store column names\n",
    "        self.cols = {\n",
    "            'date': date_col,\n",
    "            'interest_rate': interest_col,\n",
    "            'output_gap': output_gap_col,\n",
    "            'inflation': inflation_col\n",
    "        }\n",
    "        \n",
    "        # Validate dataframe\n",
    "        required_cols = list(self.cols.values())\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "        \n",
    "        # Ensure date is datetime and sort\n",
    "        self.data = df.copy()\n",
    "        \n",
    "        # Convert Period to timestamp if necessary\n",
    "        if isinstance(self.data[self.cols['date']].dtype, pd.PeriodDtype):\n",
    "            self.data[self.cols['date']] = self.data[self.cols['date']].dt.to_timestamp()\n",
    "        else:\n",
    "            self.data[self.cols['date']] = pd.to_datetime(self.data[self.cols['date']])\n",
    "\n",
    "        self.data = self.data.sort_values(self.cols['date']).reset_index(drop=True)\n",
    "        \n",
    "        # Split data\n",
    "        split_idx = int(len(self.data) * (1 - validation_split))\n",
    "        self.train_data = self.data.iloc[:split_idx].reset_index(drop=True)\n",
    "        self.val_data = self.data.iloc[split_idx:].reset_index(drop=True)\n",
    "        \n",
    "        self.lookback_periods = lookback_periods\n",
    "        self.is_validation = False\n",
    "        self.active_data = self.train_data\n",
    "        \n",
    "        # Initialize episode state\n",
    "        self.current_idx = lookback_periods\n",
    "        self.max_idx = len(self.active_data) - 1\n",
    "        \n",
    "        # Policy targets from paper\n",
    "        self.inflation_target = 2.0\n",
    "        self.output_gap_target = 0.0\n",
    "        \n",
    "        # Calculate normalization statistics from training data only\n",
    "        self.compute_normalization_stats()\n",
    "\n",
    "    def compute_normalization_stats(self) -> None:\n",
    "        \"\"\"Compute normalization statistics from training data.\"\"\"\n",
    "        self.data_stats = {\n",
    "            'inflation_mean': self.train_data[self.cols['inflation']].mean(),\n",
    "            'inflation_std': self.train_data[self.cols['inflation']].std(),\n",
    "            'output_gap_mean': self.train_data[self.cols['output_gap']].mean(),\n",
    "            'output_gap_std': self.train_data[self.cols['output_gap']].std(),\n",
    "            'interest_rate_mean': self.train_data[self.cols['interest_rate']].mean(),\n",
    "            'interest_rate_std': self.train_data[self.cols['interest_rate']].std()\n",
    "        }\n",
    "    \n",
    "    def normalize(self, value: float, variable: str) -> float:\n",
    "        \"\"\"Normalize a value using stored statistics.\"\"\"\n",
    "        return (value - self.data_stats[f'{variable}_mean']) / self.data_stats[f'{variable}_std']\n",
    "    \n",
    "    def denormalize(self, value: float, variable: str) -> float:\n",
    "        \"\"\"Denormalize a value using stored statistics.\"\"\"\n",
    "        return value * self.data_stats[f'{variable}_std'] + self.data_stats[f'{variable}_mean']\n",
    "    \n",
    "    def switch_to_validation(self) -> None:\n",
    "        \"\"\"Switch to validation dataset.\"\"\"\n",
    "        self.is_validation = True\n",
    "        self.active_data = self.val_data\n",
    "        self.current_idx = self.lookback_periods\n",
    "        self.max_idx = len(self.active_data) - 1\n",
    "    \n",
    "    def switch_to_training(self) -> None:\n",
    "        \"\"\"Switch to training dataset.\"\"\"\n",
    "        self.is_validation = False\n",
    "        self.active_data = self.train_data\n",
    "        self.current_idx = self.lookback_periods\n",
    "        self.max_idx = len(self.active_data) - 1\n",
    "    \n",
    "    def get_state(self) -> np.ndarray:\n",
    "        \"\"\"Get current state including lookback periods.\"\"\"\n",
    "        start_idx = self.current_idx - self.lookback_periods\n",
    "        end_idx = self.current_idx + 1\n",
    "        \n",
    "        state_data = {\n",
    "            'inflation': self.active_data[self.cols['inflation']].iloc[start_idx:end_idx].values,\n",
    "            'output_gap': self.active_data[self.cols['output_gap']].iloc[start_idx:end_idx].values,\n",
    "            'interest_rate': self.active_data[self.cols['interest_rate']].iloc[start_idx:end_idx-1].values\n",
    "        }\n",
    "        \n",
    "        # Create normalized state vector\n",
    "        normalized_state = []\n",
    "        \n",
    "        # Add current and lagged inflation and output gap\n",
    "        for var in ['inflation', 'output_gap']:\n",
    "            normalized_state.extend([self.normalize(x, var) for x in state_data[var]])\n",
    "            \n",
    "        # Add lagged interest rates\n",
    "        normalized_state.extend([self.normalize(x, 'interest_rate') for x in state_data['interest_rate']])\n",
    "        \n",
    "        return np.array(normalized_state)\n",
    "    \n",
    "    def compute_reward(self, inflation: float, output_gap: float) -> float:\n",
    "        \"\"\"\n",
    "        Compute reward based on paper's specification:\n",
    "        rt = -ωπ(πt+1 - π*)² - ωy(yt+1)²\n",
    "        \"\"\"\n",
    "        omega_pi = omega_y = 0.5\n",
    "        \n",
    "        inflation_loss = -omega_pi * (inflation - self.inflation_target)**2\n",
    "        output_gap_loss = -omega_y * output_gap**2\n",
    "        reward = inflation_loss + output_gap_loss\n",
    "        \n",
    "        # Additional penalty for large deviations\n",
    "        if abs(inflation - self.inflation_target) > 2:\n",
    "            reward *= 0.1\n",
    "        if abs(output_gap) > 2:\n",
    "            reward *= 0.1\n",
    "            \n",
    "        return reward\n",
    "    \n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"Reset environment to start of current dataset.\"\"\"\n",
    "        self.current_idx = self.lookback_periods\n",
    "        return self.get_state()\n",
    "    \n",
    "    def step(self, action: float) -> Tuple[np.ndarray, float, bool, Dict[str, Any]]:\n",
    "        \"\"\"Execute one step in environment.\"\"\"\n",
    "        if self.current_idx >= self.max_idx:\n",
    "            return self.get_state(), 0, True, {}\n",
    "        \n",
    "        actual_action = self.denormalize(action, 'interest_rate')\n",
    "        self.current_idx += 1\n",
    "        next_state = self.get_state()\n",
    "        \n",
    "        current_inflation = self.active_data[self.cols['inflation']].iloc[self.current_idx]\n",
    "        current_output_gap = self.active_data[self.cols['output_gap']].iloc[self.current_idx]\n",
    "        \n",
    "        reward = self.compute_reward(current_inflation, current_output_gap)\n",
    "        done = self.current_idx >= self.max_idx\n",
    "        \n",
    "        info = {\n",
    "            'date': self.active_data[self.cols['date']].iloc[self.current_idx],\n",
    "            'actual_inflation': current_inflation,\n",
    "            'actual_output_gap': current_output_gap,\n",
    "            'actual_interest_rate': actual_action,\n",
    "            'inflation_target': self.inflation_target,\n",
    "            'output_gap_target': self.output_gap_target\n",
    "        }\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "\n",
    "\n",
    "# Create environment\n",
    "env = DataBasedEconomyEnv(\n",
    "    df=merged_df,\n",
    "    date_col='Quarter',\n",
    "    interest_col='Avg_Interest_Rate',\n",
    "    output_gap_col='Output_gap (%)',\n",
    "    inflation_col='GDP Deflator'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.24641309, -2.19601752, -2.14182999,  1.49115918,  1.64195272,\n",
       "        1.81897123,  0.71984371,  0.41531225])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.19601752, -2.14182999, -2.03682717,  1.64195272,  1.81897123,\n",
       "         2.25823938,  0.41531225,  0.33917938]),\n",
       " -6.303210601250001,\n",
       " False,\n",
       " {'date': Timestamp('1988-04-01 00:00:00'),\n",
       "  'actual_inflation': 37.3205,\n",
       "  'actual_output_gap': 3.62,\n",
       "  'actual_interest_rate': 9.158091968033347,\n",
       "  'inflation_target': 2.0,\n",
       "  'output_gap_target': 0.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience tuple structure\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state'])\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 64):\n",
    "        super(Actor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.ReLU()  # ReLU for ZLB constraint (i >= 0)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights using paper's approach\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "                \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(state)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 64):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        # Observation path\n",
    "        self.obs_path = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Action path\n",
    "        self.action_path = nn.Sequential(\n",
    "            nn.Linear(action_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Common path\n",
    "        self.common_path = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "                \n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        obs_features = self.obs_path(state)\n",
    "        action_features = self.action_path(action)\n",
    "        combined = torch.cat([obs_features, action_features], dim=1)\n",
    "        return self.common_path(combined)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process noise generator\"\"\"\n",
    "    def __init__(self, size: int, mu: float = 0., theta: float = 0.15, sigma: float = 1.):\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = None\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.copy(self.mu)\n",
    "        \n",
    "    def sample(self) -> np.ndarray:\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        hidden_dim: int = 64,\n",
    "        buffer_size: int = 10000,\n",
    "        batch_size: int = 64,\n",
    "        gamma: float = 0.99,\n",
    "        tau: float = 0.001,\n",
    "        actor_lr: float = 0.0001,\n",
    "        critic_lr: float = 0.0001\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Networks\n",
    "        self.actor = Actor(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        \n",
    "        self.critic = Critic(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.critic_target = Critic(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "        # Optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        \n",
    "        # Experience replay\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Parameters\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Training metrics\n",
    "        self.critic_losses = []\n",
    "        self.actor_losses = []\n",
    "        \n",
    "    def select_action(self, state: np.ndarray, noise: np.ndarray = None) -> np.ndarray:\n",
    "        \"\"\"Select action with optional exploration noise\"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state).cpu().numpy()[0]\n",
    "            \n",
    "        if noise is not None:\n",
    "            action += noise\n",
    "            \n",
    "        return np.clip(action, 0, None)  # Apply ZLB constraint\n",
    "    \n",
    "    def store_experience(self, state: np.ndarray, action: np.ndarray, \n",
    "                        reward: float, next_state: np.ndarray) -> None:\n",
    "        \"\"\"Store experience in replay buffer\"\"\"\n",
    "        self.buffer.append(Experience(state, action, reward, next_state))\n",
    "    \n",
    "    def train(self) -> Tuple[float, float]:\n",
    "        \"\"\"Train the agent using a minibatch from replay buffer\"\"\"\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return 0.0, 0.0\n",
    "        \n",
    "        # Sample minibatch\n",
    "        batch = random.sample(self.buffer, self.batch_size)\n",
    "        state_batch = torch.FloatTensor([exp.state for exp in batch]).to(self.device)\n",
    "        action_batch = torch.FloatTensor([exp.action for exp in batch]).to(self.device)\n",
    "        reward_batch = torch.FloatTensor([exp.reward for exp in batch]).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor([exp.next_state for exp in batch]).to(self.device)\n",
    "        \n",
    "        # Update critic (Bellman equation)\n",
    "        next_actions = self.actor_target(next_state_batch)\n",
    "        target_q = reward_batch + self.gamma * self.critic_target(next_state_batch, next_actions)\n",
    "        current_q = self.critic(state_batch, action_batch)\n",
    "        critic_loss = nn.MSELoss()(current_q, target_q)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Update actor using policy gradient\n",
    "        actor_loss = -self.critic(state_batch, self.actor(state_batch)).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Update target networks\n",
    "        self._update_target_network(self.actor_target, self.actor)\n",
    "        self._update_target_network(self.critic_target, self.critic)\n",
    "        \n",
    "        # Store losses\n",
    "        self.critic_losses.append(critic_loss.item())\n",
    "        self.actor_losses.append(actor_loss.item())\n",
    "        \n",
    "        return critic_loss.item(), actor_loss.item()\n",
    "    \n",
    "    def _update_target_network(self, target: nn.Module, source: nn.Module) -> None:\n",
    "        \"\"\"Soft update target network parameters\"\"\"\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "            )\n",
    "    \n",
    "    def get_training_metrics(self) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"Return training metrics\"\"\"\n",
    "        return self.critic_losses, self.actor_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import torch\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# Experience tuple structure\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state'])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int = 10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        \n",
    "    def push(self, state: np.ndarray, action: np.ndarray, \n",
    "             reward: float, next_state: np.ndarray) -> None:\n",
    "        \"\"\"Add experience to buffer\"\"\"\n",
    "        self.buffer.append(Experience(state, action, reward, next_state))\n",
    "        \n",
    "    def sample(self, batch_size: int) -> Tuple:\n",
    "        \"\"\"Sample a batch of experiences\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state = torch.FloatTensor([exp.state for exp in batch])\n",
    "        action = torch.FloatTensor([exp.action for exp in batch])\n",
    "        reward = torch.FloatTensor([exp.reward for exp in batch])\n",
    "        next_state = torch.FloatTensor([exp.next_state for exp in batch])\n",
    "        return state, action, reward, next_state\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.buffer)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process for exploration\"\"\"\n",
    "    def __init__(self, size: int, mu: float = 0., theta: float = 0.15, sigma: float = 1.):\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = None\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.copy(self.mu)\n",
    "        \n",
    "    def sample(self) -> np.ndarray:\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "class EpisodeManager:\n",
    "    \"\"\"Manages episode execution and data collection\"\"\"\n",
    "    def __init__(self, env, agent):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.episode_rewards = []\n",
    "        self.policy_states = {}\n",
    "        \n",
    "    def run_episode(self, training: bool = True, noise_sigma: float = 1.0) -> Dict:\n",
    "        state = self.env.reset()\n",
    "        episode_reward = 0\n",
    "        transitions = []\n",
    "        noise = OUNoise(1, sigma=noise_sigma) if training else None\n",
    "        \n",
    "        while True:\n",
    "            action = self.agent.select_action(state)\n",
    "            if noise:\n",
    "                action += noise.sample()\n",
    "                \n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            \n",
    "            if training:\n",
    "                self.agent.buffer.push(state, action, reward, next_state)\n",
    "                self.agent.train()\n",
    "                \n",
    "            transitions.append({\n",
    "                'state': state,\n",
    "                'action': action,\n",
    "                'reward': reward,\n",
    "                'next_state': next_state,\n",
    "                'info': info\n",
    "            })\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        \n",
    "        return {\n",
    "            'episode_reward': episode_reward,\n",
    "            'transitions': transitions,\n",
    "            'final_info': info\n",
    "        }\n",
    "    \n",
    "    def get_metrics(self) -> Dict:\n",
    "        \"\"\"Return training metrics\"\"\"\n",
    "        return {\n",
    "            'episode_rewards': self.episode_rewards,\n",
    "            'avg_reward': np.mean(self.episode_rewards),\n",
    "            'std_reward': np.std(self.episode_rewards)\n",
    "        }\n",
    "\n",
    "class ValidationManager:\n",
    "    \"\"\"Manages validation process\"\"\"\n",
    "    def __init__(self, env, agent):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.best_reward = float('-inf')\n",
    "        self.best_policy_state = None\n",
    "        \n",
    "    def validate(self, num_episodes: int = 5) -> Dict:\n",
    "        self.env.switch_to_validation()\n",
    "        episode_manager = EpisodeManager(self.env, self.agent)\n",
    "        \n",
    "        validation_rewards = []\n",
    "        for _ in range(num_episodes):\n",
    "            episode_info = episode_manager.run_episode(training=False)\n",
    "            validation_rewards.append(episode_info['episode_reward'])\n",
    "            \n",
    "        avg_reward = np.mean(validation_rewards)\n",
    "        \n",
    "        # Save best policy\n",
    "        if avg_reward > self.best_reward:\n",
    "            self.best_reward = avg_reward\n",
    "            self.best_policy_state = {\n",
    "                'actor': self.agent.actor.state_dict(),\n",
    "                'critic': self.agent.critic.state_dict(),\n",
    "                'reward': avg_reward\n",
    "            }\n",
    "            \n",
    "        self.env.switch_to_training()\n",
    "        \n",
    "        return {\n",
    "            'avg_reward': avg_reward,\n",
    "            'std_reward': np.std(validation_rewards),\n",
    "            'best_reward': self.best_reward\n",
    "        }\n",
    "    \n",
    "    def restore_best_policy(self) -> None:\n",
    "        \"\"\"Restore best performing policy\"\"\"\n",
    "        if self.best_policy_state is not None:\n",
    "            self.agent.actor.load_state_dict(self.best_policy_state['actor'])\n",
    "            self.agent.critic.load_state_dict(self.best_policy_state['critic'])\n",
    "            print(f\"Restored best policy with validation reward: {self.best_policy_state['reward']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
