{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on paper: https://d-nb.info/1248317343/34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Output Gap Data extraction\n",
    "\n",
    "First I got the quarterly GDP for the period [Office for National Statistics]:\n",
    "https://www.ons.gov.uk/economy/grossdomesticproductgdp/timeseries/ybha/qna\n",
    "\n",
    "I got the yearly output gap [Office for Budget Responsibility (OBR)]\n",
    "https://obr.uk/public-finances-databank-2024-25/\n",
    "\n",
    "Using the quarterly estimates developed [OBR: Output gap measurement: judgement and uncertainty] I replicated the shape of the quarterly output gaps in %.\n",
    "https://obr.uk/docs/dlm_uploads/WorkingPaperNo5.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   GDP_Real (m£)  GDP_Pot (m£)  Output_gap (%) Quarter\n",
      "0         127119        130233            2.45  1987Q3\n",
      "1         129815        133288            2.68  1987Q4\n",
      "2         133283        137215            2.95  1988Q1\n",
      "3         136630        141576            3.62  1988Q2\n",
      "4         140801        145602            3.41  1988Q3\n",
      "    GDP_Real (m£)  GDP_Pot (m£)  Output_gap (%) Quarter\n",
      "77         372900        372629           -0.07  2006Q4\n",
      "78         376958        378202            0.33  2007Q1\n",
      "79         386144        387920            0.46  2007Q2\n",
      "80         389291        392366            0.79  2007Q3\n",
      "81         392244        396777            1.16  2007Q4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# Use the raw URL from the GitHub repository\n",
    "xlsx_url = \"https://raw.githubusercontent.com/guri99uy/ST449_Project/52611de9d475e711c4c917c4d5ca137427404612/outputgap.xlsx\"\n",
    "\n",
    "\n",
    "# Load the Excel file\n",
    "df_outputgap = pd.read_excel(xlsx_url, engine='openpyxl')  # Ensure you specify the 'openpyxl' engine for .xlsx files\n",
    "\n",
    "# Define a function to parse QQYYYY\n",
    "def parse_qqyyyy(qqyyyy):\n",
    "    # Extract the quarter and year\n",
    "    quarter = int(qqyyyy[1])\n",
    "    year = int(qqyyyy[2:])\n",
    "    \n",
    "    # Map the quarter to the first month of that quarter\n",
    "    quarter_start_month = {1: 1, 2: 4, 3: 7, 4: 10}\n",
    "    month = quarter_start_month[quarter]\n",
    "    \n",
    "    # Create a datetime object for the first day of the quarter\n",
    "    return dt.datetime(year, month, 1)\n",
    "\n",
    "# Apply the function to the first column 'QQYYYY' to convert it to datetime\n",
    "df_outputgap['QQYYYY'] = df_outputgap['QQYYYY'].apply(parse_qqyyyy)\n",
    "# Rename a single column, e.g., 'OldName' to 'NewName'\n",
    "df_outputgap.rename(columns={'QQYYYY': 'Date'}, inplace=True)\n",
    "\n",
    "#Get Date in Quarters\n",
    "df_outputgap['Date'] = pd.to_datetime(df_outputgap['Date'])\n",
    "df_outputgap['Quarter'] = df_outputgap['Date'].dt.to_period('Q')\n",
    "df_outputgap = df_outputgap.drop(columns=['Date'])\n",
    "\n",
    "df_outputgap['GDP_Pot (m£)'] = df_outputgap['GDP_Pot (m£)'].round(0).astype(int)\n",
    "df_outputgap['Output_gap (%)'] = df_outputgap['Output_gap (%)'].round(2)\n",
    "\n",
    "# Display the first few rows of the transformed DataFrame\n",
    "print(df_outputgap.head())\n",
    "print(df_outputgap.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Interest Rate\n",
    "Got .xlsx file from [Bank of Engalnd]\n",
    "https://www.bankofengland.co.uk/boeapps/database/Bank-Rate.asp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Every Interest rate by Bank of England:\n",
      "        Date  Interest_rate\n",
      "0 2024-11-07           4.75\n",
      "1 2024-08-01           5.00\n",
      "2 2023-08-03           5.25\n",
      "3 2023-06-22           5.00\n",
      "4 2023-05-11           4.50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# Raw URL of the Excel file\n",
    "url = \"https://raw.githubusercontent.com/guri99uy/ST449_Project/7715079b32be2ea0b9e2e77a3f7b81244f85720f/Bank_Rate.xlsx\"\n",
    "df_interest_rate = pd.read_excel(url, engine='openpyxl')\n",
    "\n",
    "\n",
    "# Rename columns for easier access (optional)\n",
    "df_interest_rate.columns = ['Date', 'Interest_rate']\n",
    "\n",
    "# Convert the 'Date_Changed' column to datetime format\n",
    "def parse_date(date_str):\n",
    "    # Handle the format '07 Nov 24' as 'DD MMM YY'\n",
    "    return dt.datetime.strptime(date_str, '%d %b %y')\n",
    "\n",
    "df_interest_rate['Date'] = df_interest_rate['Date'].apply(parse_date)\n",
    "\n",
    "# Check if 'Rate' column is string type, and process accordingly\n",
    "if df_interest_rate['Interest_rate'].dtype == 'object':\n",
    "    # Clean the 'Rate' column (replace commas with dots and convert to float)\n",
    "    df_interest_rate['Interest_rate'] = df_interest_rate['Rate'].str.replace(',', '.').astype(float)\n",
    "else:\n",
    "    # Ensure the 'Rate' column is numeric\n",
    "    df_interest_rate['Interest_rate'] = pd.to_numeric(df_interest_rate['Interest_rate'], errors='coerce')\n",
    "\n",
    "# Display the processed DataFrame\n",
    "print(\"\\nEvery Interest rate by Bank of England:\")\n",
    "print(df_interest_rate.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets process the data to: \n",
    "1. Get the quarter average\n",
    "2. Assign missing quarters with the last value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Quarter  Avg_Interest_Rate\n",
      "0  1987Q3              9.880\n",
      "1  1987Q4              8.880\n",
      "2  1988Q1              8.630\n",
      "3  1988Q2              8.080\n",
      "4  1988Q3             10.755\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_interest_rate is the DataFrame with 'Date_Changed' and 'Rate'\n",
    "# Ensure 'Date_Changed' is a datetime column\n",
    "df_interest_rate['Date'] = pd.to_datetime(df_interest_rate['Date'])\n",
    "\n",
    "# Create a column for the quarter and year as strings for grouping\n",
    "df_interest_rate['Quarter'] = df_interest_rate['Date'].dt.to_period('Q')\n",
    "\n",
    "# Group by the 'Quarter' column and calculate the average interest rate\n",
    "quarterly_avg_rate = (\n",
    "    df_interest_rate.groupby('Quarter', as_index=False)['Interest_rate']\n",
    "    .mean()\n",
    "    .rename(columns={'Interest_rate': 'Avg_Interest_Rate'})\n",
    ")\n",
    "\n",
    "full_quarters = pd.period_range('1975Q1', '2007Q4', freq='Q')\n",
    "quarterly_avg_rate['Quarter'] = pd.PeriodIndex(quarterly_avg_rate['Quarter'], freq='Q')\n",
    "quarterly_avg_rate = quarterly_avg_rate.set_index('Quarter').reindex(full_quarters)\n",
    "\n",
    "# Fill missing values with the value from the previous quarter\n",
    "quarterly_avg_rate['Avg_Interest_Rate'] = quarterly_avg_rate['Avg_Interest_Rate'].ffill()\n",
    "quarterly_avg_rate.reset_index(inplace=True)\n",
    "quarterly_avg_rate.rename(columns={'index': 'Quarter'}, inplace=True)\n",
    "\n",
    "# Filter 1997 - 2007\n",
    "Quarterly_interest_rates = quarterly_avg_rate[\n",
    "    (quarterly_avg_rate['Quarter'] >= '1987Q3') & (quarterly_avg_rate['Quarter'] <= '2007Q4')\n",
    "]\n",
    "Quarterly_interest_rates.reset_index(inplace=True)\n",
    "Quarterly_interest_rates = Quarterly_interest_rates.drop(columns=['index'])\n",
    "\n",
    "# Display\n",
    "print(Quarterly_interest_rates.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inflation\n",
    "Source?\n",
    "Relevant comments:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Quarter  GDP Deflator\n",
      "0  1987Q3       35.8724\n",
      "1  1987Q4       36.2206\n",
      "2  1988Q1       36.5950\n",
      "3  1988Q2       37.3205\n",
      "4  1988Q3       37.9849\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# GitHub raw URL for inflation\n",
    "url = \"https://raw.githubusercontent.com/guri99uy/ST449_Project/c87d1b581f0af98f2a813a9c6134160303e74883/inf_Data.csv\"\n",
    "inflation = pd.read_csv(url)\n",
    "\n",
    "# Rename columns\n",
    "inf_data = inflation.rename(columns={\"Implied GDP deflator at market prices: SA Index\": \"GDP Deflator\"})\n",
    "inf_data.rename(columns={\"Title\": \"Quarter\"}, inplace=True)\n",
    "\n",
    "# Change Quarter\n",
    "inf_data[\"Quarter\"] = inf_data[\"Quarter\"].str.replace(r\"(\\d{4})\\sQ(\\d)\", r\"\\1Q\\2\", regex=True)\n",
    "\n",
    "print(inf_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Merge relevant data\n",
    "1. Output Gap\n",
    "2. Interest rate\n",
    "3. Inflation\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Quarter  Avg_Interest_Rate  GDP_Real (m£)  GDP_Pot (m£)  Output_gap (%)  \\\n",
      "0  1987Q3              9.880         127119        130233            2.45   \n",
      "1  1987Q4              8.880         129815        133288            2.68   \n",
      "2  1988Q1              8.630         133283        137215            2.95   \n",
      "3  1988Q2              8.080         136630        141576            3.62   \n",
      "4  1988Q3             10.755         140801        145602            3.41   \n",
      "\n",
      "   GDP Deflator  \n",
      "0       35.8724  \n",
      "1       36.2206  \n",
      "2       36.5950  \n",
      "3       37.3205  \n",
      "4       37.9849  \n",
      "   Quarter  Avg_Interest_Rate  GDP_Real (m£)  GDP_Pot (m£)  Output_gap (%)  \\\n",
      "75  2006Q2               4.50         367042        366712           -0.09   \n",
      "76  2006Q3               4.75         370883        370824           -0.02   \n",
      "77  2006Q4               5.00         372900        372629           -0.07   \n",
      "78  2007Q1               5.25         376958        378202            0.33   \n",
      "79  2007Q2               5.50         386144        387920            0.46   \n",
      "\n",
      "    GDP Deflator  \n",
      "75       65.1635  \n",
      "76       65.6609  \n",
      "77       65.6670  \n",
      "78       65.7761  \n",
      "79       66.9001  \n"
     ]
    }
   ],
   "source": [
    "# Convert 'Quarter' column in all datasets to period type\n",
    "Quarterly_interest_rates['Quarter'] = pd.PeriodIndex(Quarterly_interest_rates['Quarter'], freq='Q')\n",
    "df_outputgap['Quarter'] = pd.PeriodIndex(df_outputgap['Quarter'], freq='Q')\n",
    "inf_data['Quarter'] = pd.PeriodIndex(inf_data['Quarter'], freq='Q')\n",
    "\n",
    "# Merge the datasets\n",
    "merged_df = pd.merge(Quarterly_interest_rates, df_outputgap, on='Quarter', how='inner')  # Inner join\n",
    "merged_df = pd.merge(merged_df, inf_data, on='Quarter', how='inner')  # Inner join\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(merged_df.head())\n",
    "\n",
    "\n",
    "print(merged_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stable-baselines3 gymnasium pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# Code to replicate a simplified RL approach for a Central Bank environment\n",
    "# ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# Stable-Baselines3 for RL algorithms\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# ------------------------------------\n",
    "# 1) LOAD YOUR DATA\n",
    "# ------------------------------------\n",
    "# Done before\n",
    "# ------------------------------------\n",
    "# 2) DEFINE THE CENTRAL BANK ENVIRONMENT\n",
    "# ------------------------------------\n",
    "class CentralBankEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A simplified environment where:\n",
    "      - State: (output_gap, inflation_gap, interest_rate)\n",
    "      - Action: a change in interest rate (continuous)\n",
    "      - Reward: negative of (sum of squared gaps for inflation and output)\n",
    "      \n",
    "    We'll 'walk' through historical data in order. \n",
    "    The environment does not truly simulate the effect of \n",
    "    the chosen interest rate on the next state but rather \n",
    "    replays the real-world data. \n",
    "    A more advanced approach would incorporate a small macro model \n",
    "    to determine next state from the chosen action.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, inflation_target=2.0):\n",
    "        super(CentralBankEnv, self).__init__()\n",
    "        \n",
    "        # Store historical data\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        \n",
    "        # We'll define the inflation by quarter-over-quarter changes \n",
    "        # in GDP Deflator (approx) or yoy. For simplicity, let's approximate \n",
    "        # quarterly inflation as the percent change from previous quarter's deflator:\n",
    "        self.df['Inflation'] = self.df['GDP Deflator'].pct_change() * 100\n",
    "        \n",
    "        # Fill the first inflation with 0 (or drop the first row)\n",
    "        self.df['Inflation'].fillna(0, inplace=True)\n",
    "        \n",
    "        # We'll define 'inflation_gap' = inflation - inflation_target\n",
    "        self.df['Inflation_Gap'] = self.df['Inflation'] - inflation_target\n",
    "        \n",
    "        # Episode tracking\n",
    "        self.current_step = 0\n",
    "        self.max_step = len(self.df) - 1\n",
    "        \n",
    "        # Store the target\n",
    "        self.inflation_target = inflation_target\n",
    "        \n",
    "        # Define the action space: let's assume we can change interest rate by +/- 1 percentage point\n",
    "        # from the previous quarter. If you want to allow bigger moves, change the range accordingly.\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "        \n",
    "        # Define the observation space: \n",
    "        # We'll feed the agent (Output_gap, Inflation_Gap, Current_Interest_Rate)\n",
    "        # Let's set a broad range for each dimension.\n",
    "        obs_high = np.array([ 10.0,  20.0,  30.0], dtype=np.float32)  # Arbitrary\n",
    "        obs_low  = np.array([-10.0, -20.0,   0.0], dtype=np.float32)  # Arbitrary\n",
    "        self.observation_space = spaces.Box(obs_low, obs_high, dtype=np.float32)\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Reset environment at the beginning of an episode.\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Start from a random quarter or from the beginning:\n",
    "        # Let's just start from the beginning for now.\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Return the initial observation\n",
    "        return self._get_observation(), {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take one step in the environment:\n",
    "          1. Interpret action as a change in interest rate from the current interest rate.\n",
    "          2. Move current_step to next time index.\n",
    "          3. Compute reward based on how far inflation and output gap are from the desired levels.\n",
    "        \"\"\"\n",
    "        # Current interest rate from data:\n",
    "        current_interest_rate = self.df.loc[self.current_step, 'Avg_Interest_Rate']\n",
    "        \n",
    "        # Apply the action (change)\n",
    "        delta_interest_rate = float(action[0])\n",
    "        chosen_interest_rate = current_interest_rate + delta_interest_rate\n",
    "        \n",
    "        # Move to next step\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # If we are at or beyond the end of the data, consider the episode done.\n",
    "        if self.current_step >= self.max_step:\n",
    "            done = True\n",
    "            next_obs = np.array([0.0, 0.0, 0.0], dtype=np.float32)\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            done = False\n",
    "            \n",
    "            # The real next interest rate in the dataset is historically recorded as:\n",
    "            real_next_interest_rate = self.df.loc[self.current_step, 'Avg_Interest_Rate']\n",
    "            \n",
    "            # Next output gap\n",
    "            next_output_gap = self.df.loc[self.current_step, 'Output_gap (%)']\n",
    "            \n",
    "            # Next inflation gap\n",
    "            next_inflation_gap = self.df.loc[self.current_step, 'Inflation_Gap']\n",
    "            \n",
    "            # If you wanted to simulate the effect of chosen_interest_rate on the next state,\n",
    "            # you would have a small macro model. For this example, we just use the historical data directly.\n",
    "            # next_output_gap, next_inflation_gap = macro_model(chosen_interest_rate, ...)\n",
    "            \n",
    "            # Construct the next observation\n",
    "            next_obs = np.array([next_output_gap, next_inflation_gap, real_next_interest_rate], \n",
    "                                dtype=np.float32)\n",
    "            \n",
    "            # Reward: we want to minimize the absolute gap in output and inflation\n",
    "            # For example, we can use negative sum of squares of the two gaps\n",
    "            # i.e. Reward = -[(output_gap)^2 + (inflation_gap)^2]\n",
    "            reward = - (next_output_gap**2 + next_inflation_gap**2)\n",
    "        \n",
    "        return next_obs, reward, done, False, {}\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Constructs the initial observation for the current_step.\n",
    "        \"\"\"\n",
    "        output_gap = self.df.loc[self.current_step, 'Output_gap (%)']\n",
    "        inflation_gap = self.df.loc[self.current_step, 'Inflation_Gap']\n",
    "        interest_rate = self.df.loc[self.current_step, 'Avg_Interest_Rate']\n",
    "        \n",
    "        return np.array([output_gap, inflation_gap, interest_rate], dtype=np.float32)\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Optionally implement any visualization here.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# ------------------------------------\n",
    "# 3) CREATE AN INSTANCE OF THE ENV AND WRAP IT\n",
    "# ------------------------------------\n",
    "env = CentralBankEnv(merged_df, inflation_target=2.0)\n",
    "\n",
    "# Use a DummyVecEnv to handle vectorized environments for Stable-Baselines\n",
    "# (If you want a single environment, you can just do DummyVecEnv([lambda: env]))\n",
    "vec_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# ------------------------------------\n",
    "# 4) TRAIN AN AGENT (PPO EXAMPLE)\n",
    "# ------------------------------------\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n",
    "\n",
    "# For demonstration, we will do just a few timesteps. Increase for a real run:\n",
    "model.learn(total_timesteps=1000)\n",
    "\n",
    "# ------------------------------------\n",
    "# 5) TEST / EVALUATION\n",
    "# ------------------------------------\n",
    "# We can test the learned policy by running a few episodes and collecting data.\n",
    "\n",
    "num_test_episodes = 1  # you can do more\n",
    "for ep in range(num_test_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    step_count = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "    \n",
    "    print(f\"Episode {ep+1} finished after {step_count} timesteps, total reward = {total_reward:.2f}\")\n",
    "\n",
    "# ------------------------------------\n",
    "# 6) (OPTIONAL) VISUALIZE RESULTS\n",
    "# ------------------------------------\n",
    "# Can record interest rate choices, output gap, inflation gap, etc. \n",
    "# inside the environment for plotting after training. \n",
    "# This snippet simply stores data:\n",
    "\n",
    "# Re-run an episode to store data\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "history = []\n",
    "while not done:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    next_obs, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # Store\n",
    "    history.append({\n",
    "        \"step\": env.current_step,\n",
    "        \"output_gap\": obs[0],\n",
    "        \"inflation_gap\": obs[1],\n",
    "        \"interest_rate\": obs[2],\n",
    "        \"action\": action[0],\n",
    "        \"reward\": reward\n",
    "    })\n",
    "    \n",
    "    obs = next_obs\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "history_df = pd.DataFrame(history)\n",
    "print(history_df)\n",
    "\n",
    "# Plot reward over time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_df['step'], history_df['reward'], label='Reward')\n",
    "plt.title(\"Reward Over Time for One Episode\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot interest rate and actions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_df['step'], history_df['interest_rate'], label='Interest Rate')\n",
    "plt.plot(history_df['step'], history_df['action'], label='Action (ΔInterest)')\n",
    "plt.title(\"Interest Rate and Action Over Time\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Rate (%) / Action\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DELETE BELOW, KEPT FOR MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict, Any, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBasedEconomyEnv:\n",
    "    def __init__(\n",
    "        self, \n",
    "        df: pd.DataFrame,\n",
    "        date_col: str = 'Quarter',\n",
    "        interest_col: str = 'Avg_Interest_Rate',\n",
    "        output_gap_col: str = 'Output gap (%)',\n",
    "        inflation_col: str = 'GDP Deflator',\n",
    "        lookback_periods: int = 2, \n",
    "        validation_split: float = 0.15\n",
    "    ):\n",
    "        \"\"\"Initialize environment with economic dataframe.\"\"\"\n",
    "        # Store column names\n",
    "        self.cols = {\n",
    "            'date': date_col,\n",
    "            'interest_rate': interest_col,\n",
    "            'output_gap': output_gap_col,\n",
    "            'inflation': inflation_col\n",
    "        }\n",
    "        \n",
    "        # Validate dataframe\n",
    "        required_cols = list(self.cols.values())\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "        \n",
    "        # Ensure date is datetime and sort\n",
    "        self.data = df.copy()\n",
    "        \n",
    "        # Convert Period to timestamp if necessary\n",
    "        if isinstance(self.data[self.cols['date']].dtype, pd.PeriodDtype):\n",
    "            self.data[self.cols['date']] = self.data[self.cols['date']].dt.to_timestamp()\n",
    "        else:\n",
    "            self.data[self.cols['date']] = pd.to_datetime(self.data[self.cols['date']])\n",
    "\n",
    "        self.data = self.data.sort_values(self.cols['date']).reset_index(drop=True)\n",
    "        \n",
    "        # Split data\n",
    "        split_idx = int(len(self.data) * (1 - validation_split))\n",
    "        self.train_data = self.data.iloc[:split_idx].reset_index(drop=True)\n",
    "        self.val_data = self.data.iloc[split_idx:].reset_index(drop=True)\n",
    "        \n",
    "        self.lookback_periods = lookback_periods\n",
    "        self.is_validation = False\n",
    "        self.active_data = self.train_data\n",
    "        \n",
    "        # Initialize episode state\n",
    "        self.current_idx = lookback_periods\n",
    "        self.max_idx = len(self.active_data) - 1\n",
    "        \n",
    "        # Policy targets from paper\n",
    "        self.inflation_target = 2.0\n",
    "        self.output_gap_target = 0.0\n",
    "        \n",
    "        # Calculate normalization statistics from training data only\n",
    "        self.compute_normalization_stats()\n",
    "\n",
    "    def compute_normalization_stats(self) -> None:\n",
    "        \"\"\"Compute normalization statistics from training data.\"\"\"\n",
    "        self.data_stats = {\n",
    "            'inflation_mean': self.train_data[self.cols['inflation']].mean(),\n",
    "            'inflation_std': self.train_data[self.cols['inflation']].std(),\n",
    "            'output_gap_mean': self.train_data[self.cols['output_gap']].mean(),\n",
    "            'output_gap_std': self.train_data[self.cols['output_gap']].std(),\n",
    "            'interest_rate_mean': self.train_data[self.cols['interest_rate']].mean(),\n",
    "            'interest_rate_std': self.train_data[self.cols['interest_rate']].std()\n",
    "        }\n",
    "    \n",
    "    def normalize(self, value: float, variable: str) -> float:\n",
    "        \"\"\"Normalize a value using stored statistics.\"\"\"\n",
    "        return (value - self.data_stats[f'{variable}_mean']) / self.data_stats[f'{variable}_std']\n",
    "    \n",
    "    def denormalize(self, value: float, variable: str) -> float:\n",
    "        \"\"\"Denormalize a value using stored statistics.\"\"\"\n",
    "        return value * self.data_stats[f'{variable}_std'] + self.data_stats[f'{variable}_mean']\n",
    "    \n",
    "    def switch_to_validation(self) -> None:\n",
    "        \"\"\"Switch to validation dataset.\"\"\"\n",
    "        self.is_validation = True\n",
    "        self.active_data = self.val_data\n",
    "        self.current_idx = self.lookback_periods\n",
    "        self.max_idx = len(self.active_data) - 1\n",
    "    \n",
    "    def switch_to_training(self) -> None:\n",
    "        \"\"\"Switch to training dataset.\"\"\"\n",
    "        self.is_validation = False\n",
    "        self.active_data = self.train_data\n",
    "        self.current_idx = self.lookback_periods\n",
    "        self.max_idx = len(self.active_data) - 1\n",
    "    \n",
    "    def get_state(self) -> np.ndarray:\n",
    "        \"\"\"Get current state including lookback periods.\"\"\"\n",
    "        start_idx = self.current_idx - self.lookback_periods\n",
    "        end_idx = self.current_idx + 1\n",
    "        \n",
    "        state_data = {\n",
    "            'inflation': self.active_data[self.cols['inflation']].iloc[start_idx:end_idx].values,\n",
    "            'output_gap': self.active_data[self.cols['output_gap']].iloc[start_idx:end_idx].values,\n",
    "            'interest_rate': self.active_data[self.cols['interest_rate']].iloc[start_idx:end_idx-1].values\n",
    "        }\n",
    "        \n",
    "        # Create normalized state vector\n",
    "        normalized_state = []\n",
    "        \n",
    "        # Add current and lagged inflation and output gap\n",
    "        for var in ['inflation', 'output_gap']:\n",
    "            normalized_state.extend([self.normalize(x, var) for x in state_data[var]])\n",
    "            \n",
    "        # Add lagged interest rates\n",
    "        normalized_state.extend([self.normalize(x, 'interest_rate') for x in state_data['interest_rate']])\n",
    "        \n",
    "        return np.array(normalized_state)\n",
    "    \n",
    "    def compute_reward(self, inflation: float, output_gap: float) -> float:\n",
    "        \"\"\"\n",
    "        Compute reward based on paper's specification:\n",
    "        rt = -ωπ(πt+1 - π*)² - ωy(yt+1)²\n",
    "        \"\"\"\n",
    "        omega_pi = omega_y = 0.5\n",
    "        \n",
    "        inflation_loss = -omega_pi * (inflation - self.inflation_target)**2\n",
    "        output_gap_loss = -omega_y * output_gap**2\n",
    "        reward = inflation_loss + output_gap_loss\n",
    "        \n",
    "        # Additional penalty for large deviations\n",
    "        if abs(inflation - self.inflation_target) > 2:\n",
    "            reward *= 0.1\n",
    "        if abs(output_gap) > 2:\n",
    "            reward *= 0.1\n",
    "            \n",
    "        return reward\n",
    "    \n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"Reset environment to start of current dataset.\"\"\"\n",
    "        self.current_idx = self.lookback_periods\n",
    "        return self.get_state()\n",
    "    \n",
    "    def step(self, action: float) -> Tuple[np.ndarray, float, bool, Dict[str, Any]]:\n",
    "        \"\"\"Execute one step in environment.\"\"\"\n",
    "        if self.current_idx >= self.max_idx:\n",
    "            return self.get_state(), 0, True, {}\n",
    "        \n",
    "        actual_action = self.denormalize(action, 'interest_rate')\n",
    "        self.current_idx += 1\n",
    "        next_state = self.get_state()\n",
    "        \n",
    "        current_inflation = self.active_data[self.cols['inflation']].iloc[self.current_idx]\n",
    "        current_output_gap = self.active_data[self.cols['output_gap']].iloc[self.current_idx]\n",
    "        \n",
    "        reward = self.compute_reward(current_inflation, current_output_gap)\n",
    "        done = self.current_idx >= self.max_idx\n",
    "        \n",
    "        info = {\n",
    "            'date': self.active_data[self.cols['date']].iloc[self.current_idx],\n",
    "            'actual_inflation': current_inflation,\n",
    "            'actual_output_gap': current_output_gap,\n",
    "            'actual_interest_rate': actual_action,\n",
    "            'inflation_target': self.inflation_target,\n",
    "            'output_gap_target': self.output_gap_target\n",
    "        }\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "\n",
    "\n",
    "# Create environment\n",
    "env = DataBasedEconomyEnv(\n",
    "    df=merged_df,\n",
    "    date_col='Quarter',\n",
    "    interest_col='Avg_Interest_Rate',\n",
    "    output_gap_col='Output_gap (%)',\n",
    "    inflation_col='GDP Deflator'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.24641309, -2.19601752, -2.14182999,  1.49115918,  1.64195272,\n",
       "        1.81897123,  0.71984371,  0.41531225])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.19601752, -2.14182999, -2.03682717,  1.64195272,  1.81897123,\n",
       "         2.25823938,  0.41531225,  0.33917938]),\n",
       " -6.303210601250001,\n",
       " False,\n",
       " {'date': Timestamp('1988-04-01 00:00:00'),\n",
       "  'actual_inflation': 37.3205,\n",
       "  'actual_output_gap': 3.62,\n",
       "  'actual_interest_rate': 9.158091968033347,\n",
       "  'inflation_target': 2.0,\n",
       "  'output_gap_target': 0.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience tuple structure\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state'])\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 64):\n",
    "        super(Actor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.ReLU()  # ReLU for ZLB constraint (i >= 0)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights using paper's approach\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "                \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(state)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 64):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        # Observation path\n",
    "        self.obs_path = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Action path\n",
    "        self.action_path = nn.Sequential(\n",
    "            nn.Linear(action_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Common path\n",
    "        self.common_path = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "                \n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        obs_features = self.obs_path(state)\n",
    "        action_features = self.action_path(action)\n",
    "        combined = torch.cat([obs_features, action_features], dim=1)\n",
    "        return self.common_path(combined)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process noise generator\"\"\"\n",
    "    def __init__(self, size: int, mu: float = 0., theta: float = 0.15, sigma: float = 1.):\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = None\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.copy(self.mu)\n",
    "        \n",
    "    def sample(self) -> np.ndarray:\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        hidden_dim: int = 64,\n",
    "        buffer_size: int = 10000,\n",
    "        batch_size: int = 64,\n",
    "        gamma: float = 0.99,\n",
    "        tau: float = 0.001,\n",
    "        actor_lr: float = 0.0001,\n",
    "        critic_lr: float = 0.0001\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Networks\n",
    "        self.actor = Actor(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        \n",
    "        self.critic = Critic(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.critic_target = Critic(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "        # Optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        \n",
    "        # Experience replay\n",
    "        self.buffer = ReplayBuffer(capacity=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Parameters\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Training metrics\n",
    "        self.critic_losses = []\n",
    "        self.actor_losses = []\n",
    "        \n",
    "    def store_experience(self, state, action, reward, next_state):\n",
    "        \"\"\"Store experience in replay buffer\"\"\"\n",
    "        self.buffer.push(state, action, reward, next_state)\n",
    "\n",
    "    def select_action(self, state: np.ndarray, noise: np.ndarray = None) -> np.ndarray:\n",
    "        \"\"\"Select action with optional exploration noise\"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state).cpu().numpy()[0]\n",
    "            \n",
    "        if noise is not None:\n",
    "            action += noise\n",
    "            \n",
    "        return np.clip(action, 0, None)  # Apply ZLB constraint\n",
    "    \n",
    "    def train(self) -> Tuple[float, float]:\n",
    "        \"\"\"Train the agent using a minibatch from replay buffer\"\"\"\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return 0.0, 0.0\n",
    "        \n",
    "        batch = random.sample(self.buffer, self.batch_size)\n",
    "\n",
    "        # Sample minibatch\n",
    "        state_batch = torch.FloatTensor([exp.state for exp in batch]).to(self.device)\n",
    "        action_batch = torch.FloatTensor([exp.action for exp in batch]).to(self.device)\n",
    "        reward_batch = torch.FloatTensor([exp.reward for exp in batch]).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor([exp.next_state for exp in batch]).to(self.device)\n",
    "        \n",
    "        # Update critic (Bellman equation)\n",
    "        next_actions = self.actor_target(next_state_batch)\n",
    "        target_q = reward_batch + self.gamma * self.critic_target(next_state_batch, next_actions)\n",
    "        current_q = self.critic(state_batch, action_batch)\n",
    "        critic_loss = nn.MSELoss()(current_q, target_q)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Update actor using policy gradient\n",
    "        actor_loss = -self.critic(state_batch, self.actor(state_batch)).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Update target networks\n",
    "        self._update_target_network(self.actor_target, self.actor)\n",
    "        self._update_target_network(self.critic_target, self.critic)\n",
    "        \n",
    "        # Store losses\n",
    "        self.critic_losses.append(critic_loss.item())\n",
    "        self.actor_losses.append(actor_loss.item())\n",
    "        \n",
    "        return critic_loss.item(), actor_loss.item()\n",
    "    \n",
    "    def _update_target_network(self, target: nn.Module, source: nn.Module) -> None:\n",
    "        \"\"\"Soft update target network parameters\"\"\"\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "            )\n",
    "    \n",
    "    def get_training_metrics(self) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"Return training metrics\"\"\"\n",
    "        return self.critic_losses, self.actor_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(state_dim=8, action_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = Critic(state_dim=8, action_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ounoise = OUNoise(size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ReplayBuffer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ddpgagent \u001b[38;5;241m=\u001b[39m DDPGAgent(state_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, action_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 115\u001b[0m, in \u001b[0;36mDDPGAgent.__init__\u001b[1;34m(self, state_dim, action_dim, hidden_dim, buffer_size, batch_size, gamma, tau, actor_lr, critic_lr)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mcritic_lr)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Experience replay\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m ReplayBuffer(capacity\u001b[38;5;241m=\u001b[39mbuffer_size)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Parameters\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ReplayBuffer' is not defined"
     ]
    }
   ],
   "source": [
    "ddpgagent = DDPGAgent(state_dim=8, action_dim=1, buffer_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences from memory.\"\"\"\n",
    "        batch = random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "        states, actions, rewards, next_states = zip(*batch)\n",
    "        \n",
    "        return (np.array(states), \n",
    "                np.array(actions), \n",
    "                np.array(rewards, dtype=np.float32), \n",
    "                np.array(next_states))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.buffer)\n",
    "\n",
    "class EpisodeManager:\n",
    "    def __init__(self, env, agent):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        \n",
    "    def run_episode(self, training=True):\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        transitions = []\n",
    "        \n",
    "        while not done:\n",
    "            action = self.agent.select_action(state)\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            \n",
    "            if training:\n",
    "                # Now using the proper push method from our ReplayBuffer class\n",
    "                self.agent.buffer.push(state, action, reward, next_state)\n",
    "                self.agent.train()\n",
    "            \n",
    "            transitions.append({\n",
    "                'state': state,\n",
    "                'action': action,\n",
    "                'reward': reward,\n",
    "                'next_state': next_state,\n",
    "                'done': done,\n",
    "                'info': info\n",
    "            })\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        return {\n",
    "            'total_reward': total_reward,\n",
    "            'transitions': transitions\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationManager:\n",
    "    \"\"\"Manages validation process\"\"\"\n",
    "    def __init__(self, env, agent):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.best_reward = float('-inf')\n",
    "        self.best_policy_state = None\n",
    "        \n",
    "    def validate(self, num_episodes: int = 5) -> Dict:\n",
    "        self.env.switch_to_validation()\n",
    "        episode_manager = EpisodeManager(self.env, self.agent)\n",
    "        \n",
    "        validation_rewards = []\n",
    "        for _ in range(num_episodes):\n",
    "            episode_info = episode_manager.run_episode(training=False)\n",
    "            validation_rewards.append(episode_info['episode_reward'])\n",
    "            \n",
    "        avg_reward = np.mean(validation_rewards)\n",
    "        \n",
    "        # Save best policy\n",
    "        if avg_reward > self.best_reward:\n",
    "            self.best_reward = avg_reward\n",
    "            self.best_policy_state = {\n",
    "                'actor': self.agent.actor.state_dict(),\n",
    "                'critic': self.agent.critic.state_dict(),\n",
    "                'reward': avg_reward\n",
    "            }\n",
    "            \n",
    "        self.env.switch_to_training()\n",
    "        \n",
    "        return {\n",
    "            'avg_reward': avg_reward,\n",
    "            'std_reward': np.std(validation_rewards),\n",
    "            'best_reward': self.best_reward\n",
    "        }\n",
    "    \n",
    "    def restore_best_policy(self) -> None:\n",
    "        \"\"\"Restore best performing policy\"\"\"\n",
    "        if self.best_policy_state is not None:\n",
    "            self.agent.actor.load_state_dict(self.best_policy_state['actor'])\n",
    "            self.agent.critic.load_state_dict(self.best_policy_state['critic'])\n",
    "            print(f\"Restored best policy with validation reward: {self.best_policy_state['reward']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(capacity=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_manager = EpisodeManager(env, ddpgagent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_manager = ValidationManager(env, ddpgagent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Population must be a sequence.  For dicts or sets, use sorted(d).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m episode_manager\u001b[38;5;241m.\u001b[39mrun_episode(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[103], line 47\u001b[0m, in \u001b[0;36mEpisodeManager.run_episode\u001b[1;34m(self, training)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# Now using the proper push method from our ReplayBuffer class\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mpush(state, action, reward, next_state)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     49\u001b[0m transitions\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m: state,\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m: action,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m'\u001b[39m: info\n\u001b[0;32m     56\u001b[0m })\n\u001b[0;32m     58\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[98], line 146\u001b[0m, in \u001b[0;36mDDPGAgent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m--> 146\u001b[0m batch \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Sample minibatch\u001b[39;00m\n\u001b[0;32m    149\u001b[0m state_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor([exp\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m batch])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\random.py:439\u001b[0m, in \u001b[0;36mRandom.sample\u001b[1;34m(self, population, k, counts)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;66;03m# Sampling without replacement entails tracking either potential\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;66;03m# selections (the pool) in a list or previous selections in a set.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# too many calls to _randbelow(), making them slower and\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;66;03m# causing them to eat more entropy than necessary.\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(population, _Sequence):\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPopulation must be a sequence.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor dicts or sets, use sorted(d).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    441\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(population)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m counts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: Population must be a sequence.  For dicts or sets, use sorted(d)."
     ]
    }
   ],
   "source": [
    "episode_manager.run_episode(training=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
