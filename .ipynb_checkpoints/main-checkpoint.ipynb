{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DataBasedEconomyEnv:\n",
    "    def __init__(self, data_path, lookback_periods=2):\n",
    "        \"\"\"\n",
    "        Environment using real economic data\n",
    "        \n",
    "        Args:\n",
    "            data_path: Path to CSV file with columns ['date', 'inflation', 'output_gap', 'interest_rate']\n",
    "            lookback_periods: Number of previous periods to include in state\n",
    "        \"\"\"\n",
    "        # Load and preprocess data\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.data['date'] = pd.to_datetime(self.data['date'])\n",
    "        self.data = self.data.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        # Validate data columns\n",
    "        required_cols = ['inflation', 'output_gap', 'interest_rate']\n",
    "        if not all(col in self.data.columns for col in required_cols):\n",
    "            raise ValueError(f\"Data must contain columns: {required_cols}\")\n",
    "        \n",
    "        self.lookback_periods = lookback_periods\n",
    "        self.current_idx = lookback_periods\n",
    "        self.max_idx = len(self.data) - 1\n",
    "        \n",
    "        # Target values (can be modified based on policy goals)\n",
    "        self.inflation_target = 2.0\n",
    "        self.output_gap_target = 0.0\n",
    "        \n",
    "        # Store data statistics for normalization\n",
    "        self.data_stats = {\n",
    "            'inflation_mean': self.data['inflation'].mean(),\n",
    "            'inflation_std': self.data['inflation'].std(),\n",
    "            'output_gap_mean': self.data['output_gap'].mean(),\n",
    "            'output_gap_std': self.data['output_gap'].std(),\n",
    "            'interest_rate_mean': self.data['interest_rate'].mean(),\n",
    "            'interest_rate_std': self.data['interest_rate'].std()\n",
    "        }\n",
    "    \n",
    "    def normalize_data(self, data, variable):\n",
    "        \"\"\"Normalize data using stored statistics\"\"\"\n",
    "        return (data - self.data_stats[f'{variable}_mean']) / self.data_stats[f'{variable}_std']\n",
    "    \n",
    "    def denormalize_data(self, data, variable):\n",
    "        \"\"\"Denormalize data using stored statistics\"\"\"\n",
    "        return data * self.data_stats[f'{variable}_std'] + self.data_stats[f'{variable}_mean']\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"Get current state including lookback periods\"\"\"\n",
    "        start_idx = self.current_idx - self.lookback_periods\n",
    "        end_idx = self.current_idx + 1\n",
    "        \n",
    "        state_data = {\n",
    "            'inflation': self.data['inflation'].iloc[start_idx:end_idx].values,\n",
    "            'output_gap': self.data['output_gap'].iloc[start_idx:end_idx].values,\n",
    "            'interest_rate': self.data['interest_rate'].iloc[start_idx:end_idx-1].values\n",
    "        }\n",
    "        \n",
    "        # Normalize data\n",
    "        normalized_state = []\n",
    "        for var in ['inflation', 'output_gap']:\n",
    "            normalized_state.extend(self.normalize_data(state_data[var], var))\n",
    "        for var in ['interest_rate']:\n",
    "            normalized_state.extend(self.normalize_data(state_data[var], var))\n",
    "            \n",
    "        return np.array(normalized_state)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset to start of data (after lookback periods)\"\"\"\n",
    "        self.current_idx = self.lookback_periods\n",
    "        return self.get_state()\n",
    "    \n",
    "    def compute_reward(self, inflation, output_gap):\n",
    "        \"\"\"Compute reward based on paper's specification\"\"\"\n",
    "        omega_pi = omega_y = 0.5  # Equal weights as per paper\n",
    "        \n",
    "        # Basic quadratic loss\n",
    "        inflation_loss = -omega_pi * (inflation - self.inflation_target)**2\n",
    "        output_gap_loss = -omega_y * output_gap**2\n",
    "        reward = inflation_loss + output_gap_loss\n",
    "        \n",
    "        # Additional penalty for large deviations\n",
    "        if abs(inflation - self.inflation_target) > 2:\n",
    "            reward *= 0.1\n",
    "        if abs(output_gap) > 2:\n",
    "            reward *= 0.1\n",
    "            \n",
    "        return reward\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute one step using historical data\n",
    "        \n",
    "        Args:\n",
    "            action: Normalized interest rate decision\n",
    "            \n",
    "        Returns:\n",
    "            next_state: Next state\n",
    "            reward: Reward value\n",
    "            done: Whether episode is finished\n",
    "            info: Additional information\n",
    "        \"\"\"\n",
    "        if self.current_idx >= self.max_idx:\n",
    "            return self.get_state(), 0, True, {}\n",
    "        \n",
    "        # Denormalize action (interest rate decision)\n",
    "        actual_action = self.denormalize_data(action, 'interest_rate')\n",
    "        \n",
    "        # Move to next time step\n",
    "        self.current_idx += 1\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = self.get_state()\n",
    "        \n",
    "        # Get actual values for reward computation\n",
    "        current_inflation = self.data['inflation'].iloc[self.current_idx]\n",
    "        current_output_gap = self.data['output_gap'].iloc[self.current_idx]\n",
    "        \n",
    "        # Compute reward\n",
    "        reward = self.compute_reward(current_inflation, current_output_gap)\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = self.current_idx >= self.max_idx\n",
    "        \n",
    "        # Additional info for monitoring\n",
    "        info = {\n",
    "            'actual_inflation': current_inflation,\n",
    "            'actual_output_gap': current_output_gap,\n",
    "            'actual_interest_rate': actual_action,\n",
    "            'date': self.data['date'].iloc[self.current_idx]\n",
    "        }\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "\n",
    "def main():\n",
    "    # Example usage with dataset\n",
    "    env = DataBasedEconomyEnv(\n",
    "        data_path='economic_data.csv',\n",
    "        lookback_periods=2\n",
    "    )\n",
    "    \n",
    "    # Get state dimension from environment\n",
    "    state_dim = len(env.reset())\n",
    "    action_dim = 1\n",
    "    \n",
    "    # Initialize agent\n",
    "    agent = DDPGAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_dim=64,\n",
    "        buffer_size=10000,\n",
    "        batch_size=64,\n",
    "        gamma=0.99,\n",
    "        tau=0.001\n",
    "    )\n",
    "    \n",
    "    # Training parameters\n",
    "    num_episodes = 500\n",
    "    noise = OUNoise(action_dim)\n",
    "    \n",
    "    # Lists to store results\n",
    "    episode_rewards = []\n",
    "    inflation_history = []\n",
    "    output_gap_history = []\n",
    "    interest_rate_history = []\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        noise.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action with exploration noise\n",
    "            action = agent.select_action(state)\n",
    "            action = action + noise.sample()\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store experience and train\n",
    "            agent.store_experience(state, action, reward, next_state)\n",
    "            agent.train()\n",
    "            \n",
    "            # Update state and accumulate reward\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Store history\n",
    "            inflation_history.append(info['actual_inflation'])\n",
    "            output_gap_history.append(info['actual_output_gap'])\n",
    "            interest_rate_history.append(info['actual_interest_rate'])\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(f\"Episode {episode + 1}/{num_episodes}, Reward: {episode_reward:.2f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    plot_training_results(\n",
    "        episode_rewards,\n",
    "        inflation_history,\n",
    "        output_gap_history,\n",
    "        interest_rate_history\n",
    "    )\n",
    "    \n",
    "    return agent, env\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_agent, trained_env = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
